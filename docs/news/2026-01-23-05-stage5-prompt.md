# Stage 5: Final Testing and Optimization

**Files:** `backend/news-admin/index.py`, database  
**Priority:** High  
**Complexity:** Medium  
**Estimated effort:** Testing and minor fixes

---

## üéØ Stage Goal

Run comprehensive end-to-end testing of the entire system, optimize performance, fix discovered bugs, and verify that all functionality behaves correctly.

---

## üìã Tasks

1. ‚úÖ Test the full news update pipeline
2. ‚úÖ Validate content extraction and translation quality
3. ‚úÖ Optimize performance
4. ‚úÖ Verify news sorting (newest first)
5. ‚úÖ Create usage documentation

---

## üîß AI Agent Prompt

```
CONTEXT:
Stages 1-4 are complete. The system must:
- Pull content from RSS feeds and web pages
- Translate via Ollama
- Clean HTML and metadata
- Persist to the database with correct sorting

TASK:

1. Append a function for end-to-end testing at the end of the file:

```python
def test_news_pipeline():
    """
    Tests the entire news processing pipeline.
    Use for debugging: python -c "from backend.news_admin.index import test_news_pipeline; test_news_pipeline()"
    """
    print("=" * 60)
    print("NEWS PIPELINE TESTING")
    print("=" * 60)
    
    # 1. Check Ollama
    print("\n1. Checking Ollama...")
    if check_ollama_available():
        print("‚úì Ollama is available")
    else:
        print("‚úó Ollama is unavailable - run: ollama serve")
        return
    
    # 2. Translation test
    print("\n2. Translation test...")
    test_text = "This is a test article about web development and Python programming."
    translated = translate_text(test_text)
    print(f"Original: {test_text}")
    print(f"Translation: {translated}")
    if translated and translated != test_text:
        print("‚úì Translation works")
    else:
        print("‚úó Translation failed")
    
    # 3. HTML cleaning test
    print("\n3. HTML cleaning test...")
    test_html = "<p>Test content</p><script>alert('test');</script>Article URL: https://test.com"
    cleaned = clean_html(test_html)
    print(f"HTML: {test_html}")
    print(f"Cleaned: {cleaned}")
    if 'script' not in cleaned and 'Article URL' not in cleaned:
        print("‚úì Cleaning works")
    else:
        print("‚úó Cleaning failed")
    
    # 4. RSS extraction test
    print("\n4. RSS extraction test...")
    try:
        news_items = fetch_and_translate_news()
        print(f"News items fetched: {len(news_items)}")
        
        if news_items:
            first = news_items[0]
            print(f"\nSample news:")
            print(f"  Title: {first['translated_title'][:60]}...")
            print(f"  Original content length: {len(first['original_content'])} chars")
            print(f"  Translated content length: {len(first['translated_content'])} chars")
            
            if len(first['original_content']) > 100:
                print("‚úì Content extraction works")
            else:
                print("‚úó Content too short")
        else:
            print("‚úó No news fetched")
    except Exception as e:
        print(f"‚úó Error: {e}")
    
    print("\n" + "=" * 60)
    print("TESTING COMPLETE")
    print("=" * 60)
```

2. Improve database persistence by updating `save_news_to_db`:

```python
# At the start of save_news_to_db, after obtaining the cursor:

# Optimization: use batch insert for new records
new_items = []
update_items = []

# First fetch all links in a single query
links = [item['link'] for item in news_items]
cur.execute(
    "SELECT link, id, translated_content FROM news WHERE link = ANY(%s)",
    (links,)
)
existing_map = {row[0]: (row[1], row[2]) for row in cur.fetchall()}

for item in news_items:
    if item['link'] in existing_map:
        news_id, trans_content = existing_map[item['link']]
        # Update only if translated content is empty
        if not trans_content or trans_content.strip() == '':
            update_items.append((item, news_id))
    else:
        new_items.append(item)

print(f"New records: {len(new_items)}, updates: {len(update_items)}")

# Batch insert for new records
if new_items:
    insert_query = """INSERT INTO news
        (original_title, translated_title, original_excerpt, translated_excerpt,
         original_content, translated_content, source, source_url, link, image_url, category,
         published_date, translated_at, created_at, updated_at, is_active)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, TRUE)"""
    
    insert_data = [
        (
            item['original_title'], item['translated_title'],
            item['original_excerpt'], item['translated_excerpt'],
            item['original_content'], item['translated_content'],
            item['source'], item['source_url'], item['link'],
            normalize_image_url(item.get('image_url', '')),
            item['category'], item['published_date']
        )
        for item in new_items
    ]
    
    cur.executemany(insert_query, insert_data)
    inserted = len(new_items)

# Batch update for existing records
if update_items:
    update_query = """UPDATE news SET
        original_title = %s, translated_title = %s,
        original_excerpt = %s, translated_excerpt = %s,
        original_content = %s, translated_content = %s,
        source = %s, source_url = %s, image_url = %s,
        category = %s, published_date = %s,
        updated_at = CURRENT_TIMESTAMP, translated_at = CURRENT_TIMESTAMP
        WHERE id = %s"""
    
    update_data = [
        (
            item['original_title'], item['translated_title'],
            item['original_excerpt'], item['translated_excerpt'],
            item['original_content'], item['translated_content'],
            item['source'], item['source_url'],
            normalize_image_url(item.get('image_url', '')),
            item['category'], item['published_date'], news_id
        )
        for item, news_id in update_items
    ]
    
    cur.executemany(update_query, update_data)
    updated = len(update_items)
```

3. Add a function to verify sorting:

```python
def verify_news_sorting():
    """Ensures news records are sorted correctly in the database"""
    conn = get_db_connection()
    cur = conn.cursor()
    
    cur.execute("""
        SELECT id, original_title, published_date, created_at
        FROM news
        WHERE is_active = TRUE
        ORDER BY published_date DESC, created_at DESC
        LIMIT 10
    """)
    
    rows = cur.fetchall()
    print("\nSorting check (top 10 active news):")
    print("-" * 80)
    
    for i, row in enumerate(rows, 1):
        news_id, title, pub_date, created = row
        print(f"{i}. [{pub_date}] {title[:50]}...")
    
    cur.close()
    conn.close()
    
    print("-" * 80)
    print("‚úì News sorted from newest to oldest")
```

4. Create a cleanup helper:

```python
def cleanup_old_news(days_to_keep: int = 90):
    """
    Deactivates old news entries.
    
    Args:
        days_to_keep: Number of days to keep news active
    """
    conn = get_db_connection()
    cur = conn.cursor()
    
    cur.execute("""
        UPDATE news
        SET is_active = FALSE, updated_at = CURRENT_TIMESTAMP
        WHERE published_date < CURRENT_TIMESTAMP - INTERVAL '%s days'
        AND is_active = TRUE
        RETURNING id
    """, (days_to_keep,))
    
    deactivated = cur.rowcount
    conn.commit()
    cur.close()
    conn.close()
    
    print(f"Deactivated old news: {deactivated}")
    return deactivated
```

5. Add performance monitoring:

```python
import time
from functools import wraps

def timing_decorator(func):
    """Decorator to measure execution time"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"‚è± {func.__name__} finished in {elapsed:.2f} sec")
        return result
    return wrapper

# Apply to core functions:
@timing_decorator
def fetch_and_translate_news():
    # ... existing logic ...

@timing_decorator
def fetch_article_content(url, timeout=10):
    # ... existing logic ...

@timing_decorator
def translate_long_text(text, chunk_size=1500):
    # ... existing logic ...
```

REQUIREMENTS:
- All tests must pass
- Performance: processing 10 news items < 3 minutes
- Content: at least 500 characters per article
- Sorting: newest items first
- Logging: verbosely for debugging

TESTING:

1. Run the test helper:
```bash
python -c "from backend.news_admin.index import test_news_pipeline; test_news_pipeline()"
```

2. Trigger an update via API:
```bash
curl -X POST http://localhost:8000/api/news-admin \
  -H "Authorization: Bearer YOUR_TOKEN"
```

3. Validate sorting:
```bash
python -c "from backend.news_admin.index import verify_news_sorting; verify_news_sorting()"
```

4. Inspect the database:
```sql
-- News statistics
SELECT 
    COUNT(*) as total,
    COUNT(CASE WHEN LENGTH(original_content) > 500 THEN 1 END) as with_content,
    COUNT(CASE WHEN LENGTH(translated_content) > 500 THEN 1 END) as with_translation,
    AVG(LENGTH(original_content)) as avg_orig_len,
    AVG(LENGTH(translated_content)) as avg_trans_len
FROM news
WHERE is_active = TRUE;

-- Top 5 newest articles
SELECT 
    id,
    translated_title,
    published_date,
    LENGTH(translated_content) as content_len
FROM news
WHERE is_active = TRUE
ORDER BY published_date DESC, created_at DESC
LIMIT 5;
```

EXPECTED RESULTS:
- ‚úì All tests pass
- ‚úì Content length > 500 characters
- ‚úì Translation quality is acceptable
- ‚úì Sorting is correct
- ‚úì Performance is within bounds
```

---

## ‚úÖ Success Criteria

1. `test_news_pipeline()` passes every check
2. DB optimization works (batch operations)
3. `verify_news_sorting()` confirms correct ordering
4. `cleanup_old_news()` prunes outdated records
5. Performance monitoring demonstrates acceptable timings

---

## üß™ Final Testing

### Checklist
- [ ] Ollama is running and reachable
- [ ] RSS feeds parse correctly
- [ ] Web scraping works for Hacker News
- [ ] Content is cleansed from HTML and metadata
- [ ] Translation works and is high-quality
- [ ] Data persists correctly in the database
- [ ] Sorting shows newest items first
- [ ] Frontend renders news properly
- [ ] Performance is satisfactory

### Verification commands
```bash
# 1. Ollama check
curl http://localhost:11434/api/tags

# 2. Run tests
python -c "from backend.news_admin.index import test_news_pipeline; test_news_pipeline()"

# 3. Trigger news update
curl -X POST http://localhost:8000/api/news-admin \
  -H "Authorization: Bearer YOUR_TOKEN"

# 4. Sorting check
python -c "from backend.news_admin.index import verify_news_sorting; verify_news_sorting()"

# 5. Cleanup old news (optional)
python -c "from backend.news_admin.index import cleanup_old_news; cleanup_old_news(90)"
```

---

## üìä Success Metrics

### Performance
- Processing one news item: < 10 seconds
- Processing ten news items: < 3 minutes
- Translating 1000 characters: < 5 seconds

### Quality
- Content > 500 characters: 100%
- Translation without artifacts: 100%
- Clean text (no HTML): 100%

### Functionality
- Sorting is correct: ‚úì
- No duplicates created: ‚úì
- Old news refreshed: ‚úì

---

## üìù User Documentation

### How to use

1. **Start Ollama:**
```bash
ollama serve
ollama pull llama3.2
```

2. **Update news via admin panel:**
   - Log in to the admin panel
   - Navigate to "Content" ‚Üí "News"
   - Click the "Refresh Feed" button
   - Wait for completion (1-3 minutes)

3. **Verify results:**
   - Newly fetched news appears in the list
   - Newest items appear at the top
   - Each article includes full text

### Troubleshooting

**Issue:** Ollama unavailable  
**Fix:** Launch `ollama serve` in a separate terminal

**Issue:** Translation fails  
**Fix:** Check model with `ollama list | grep llama3.2`

**Issue:** Slow performance  
**Fix:** Reduce number of sources or raise limits

---

## üéâ Completion

Once all tests pass, the system is ready for production!

**What was fixed:**
1. ‚úÖ Content extraction from RSS feeds and web pages
2. ‚úÖ Quality translation via Ollama
3. ‚úÖ HTML/metadata cleanup
4. ‚úÖ Correct news sorting
5. ‚úÖ Performance optimizations

**Next steps:**
- Monitor the system in production
- Add new news sources
- Automate updates via cron

---

**Note:** Save all prompts for future system improvements.

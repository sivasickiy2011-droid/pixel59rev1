# –≠—Ç–∞–ø 2: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ Web Scraping –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

**–§–∞–π–ª –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:** `backend/news-admin/index.py`  
**–ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** `requests`, `beautifulsoup4`, `readability-lxml`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –í—ã—Å–æ–∫–∏–π  
**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** –í—ã—Å–æ–∫–∞—è  
**–ü—Ä–∏–º–µ—Ä–Ω—ã–π –æ–±—ä–µ–º:** ~200 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞

---

## üéØ –¶–µ–ª—å —ç—Ç–∞–ø–∞

–î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Å—Å—ã–ª–∫–µ, –∫–æ–≥–¥–∞ RSS-–ª–µ–Ω—Ç–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

---

## üìã –ó–∞–¥–∞—á–∏

1. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `requirements.txt` —Å –Ω–æ–≤—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `fetch_article_content(url)` –¥–ª—è web scraping
3. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å scraping –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
4. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ —Ç–∞–π–º–∞—É—Ç—ã
5. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å fallback-—Å—Ç—Ä–∞—Ç–µ–≥–∏—é

---

## üîß –ü—Ä–æ–º–ø—Ç –¥–ª—è –ò–ò –∞–≥–µ–Ω—Ç–∞

```
–ö–û–ù–¢–ï–ö–°–¢:
–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–æ–≤–æ—Å—Ç–µ–π. –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω - —Ç–µ–ø–µ—Ä—å –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ RSS.
–ù–æ –º–Ω–æ–≥–∏–µ RSS-–ª–µ–Ω—Ç—ã (–æ—Å–æ–±–µ–Ω–Ω–æ Hacker News) —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –≤–Ω–µ—à–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.

–ó–ê–î–ê–ù–ò–ï:

1. –û–±–Ω–æ–≤–∏ —Ñ–∞–π–ª backend/news-admin/requirements.txt, –¥–æ–±–∞–≤—å –≤ –∫–æ–Ω–µ—Ü:

```
requests==2.31.0
beautifulsoup4==4.12.2
readability-lxml==0.8.1
lxml==4.9.3
```

2. –î–æ–±–∞–≤—å –∏–º–ø–æ—Ä—Ç—ã –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ backend/news-admin/index.py (–ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ 10):

```python
import requests
from bs4 import BeautifulSoup
from readability import Document
import time
```

3. –°–æ–∑–¥–∞–π –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é fetch_article_content(url) –ü–û–°–õ–ï —Ñ—É–Ω–∫—Ü–∏–∏ extract_full_content():

```python
def fetch_article_content(url: str, timeout: int = 10) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ URL.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç readability –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    
    Args:
        url: URL —Å—Ç–∞—Ç—å–∏
        timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        print(f"Fetching article from: {url}")
        
        # –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = response.headers.get('Content-Type', '')
        if 'text/html' not in content_type:
            print(f"Skipping non-HTML content: {content_type}")
            return ''
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é readability
        doc = Document(response.content)
        article_html = doc.summary()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ HTML —Å BeautifulSoup
        soup = BeautifulSoup(article_html, 'lxml')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–æ–≤, —Å—Ç–∏–ª–µ–π –∏ –¥—Ä—É–≥–∏—Ö –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
            element.decompose()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text = soup.get_text(separator=' ', strip=True)
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())
        
        print(f"Extracted {len(text)} characters from article")
        
        return text
        
    except requests.Timeout:
        print(f"Timeout fetching article: {url}")
        return ''
    except requests.RequestException as e:
        print(f"Error fetching article {url}: {e}")
        return ''
    except Exception as e:
        print(f"Unexpected error parsing article {url}: {e}")
        return ''
```

4. –°–æ–∑–¥–∞–π –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é should_fetch_from_web() –ü–û–°–õ–ï fetch_article_content():

```python
def should_fetch_from_web(content: str, min_length: int = 500) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    Args:
        content: –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ RSS
        min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    
    Returns:
        True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
    if not content or len(content.strip()) < min_length:
        return True
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫/–æ–ø–∏—Å–∞–Ω–∏–µ
    # (–æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    sentences = content.count('.') + content.count('!') + content.count('?')
    if sentences < 3:
        return True
    
    return False
```

5. –û–±–Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏—é fetch_and_translate_news(), –∑–∞–º–µ–Ω–∏ –±–ª–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ —Å extract_full_content):

```python
# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ RSS
original_content_raw = extract_full_content(entry)
original_content = clean_html(original_content_raw)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ RSS
if should_fetch_from_web(original_content):
    print(f"RSS content insufficient ({len(original_content)} chars), fetching from web...")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    web_content = fetch_article_content(entry.link)
    
    if web_content and len(web_content) > len(original_content):
        print(f"Using web content ({len(web_content)} chars) instead of RSS ({len(original_content)} chars)")
        original_content = clean_html(web_content)
    else:
        print(f"Web scraping failed or returned less content, using RSS")
    
    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –≤–µ–∂–ª–∏–≤–æ—Å—Ç–∏
    time.sleep(1)
else:
    print(f"RSS content sufficient ({len(original_content)} chars), skipping web fetch")

# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
content_to_translate = original_content[:5000] if len(original_content) > 5000 else original_content

# –ü–µ—Ä–µ–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
if len(content_to_translate) > 50:
    translated_content_raw = translate_full_content(content_to_translate)
    translated_content = clean_html(translated_content_raw)
else:
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º excerpt
    translated_content = excerpt_ru
    original_content = clean_summary

print(f'Final original content: {len(original_content)} chars')
print(f'Final translated content: {len(translated_content)} chars')
```

6. –î–æ–±–∞–≤—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ web-–∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ fetch_and_translate_news():

```python
# –í –Ω–∞—á–∞–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏, –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è feeds
all_news = []
web_fetch_count = 0
MAX_WEB_FETCHES = 5  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

print(f"Starting to fetch news from {len(feeds)} feeds...")

for feed_info in feeds:
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    for entry in feed.entries[:limit]:
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ...
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç web-–∑–∞–ø—Ä–æ—Å–æ–≤
        if should_fetch_from_web(original_content) and web_fetch_count < MAX_WEB_FETCHES:
            web_content = fetch_article_content(entry.link)
            web_fetch_count += 1
            # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
```

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –£—Å—Ç–∞–Ω–æ–≤–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests beautifulsoup4 readability-lxml lxml
- –î–æ–±–∞–≤—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—à–∏–±–æ–∫
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏–π
- –î–æ–±–∞–≤—å –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (rate limiting)
- –õ–æ–≥–∏—Ä—É–π –≤—Å–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:
–°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ RSS, –∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏.

–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:
1. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ Hacker News (–æ–±—ã—á–Ω–æ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ RSS)
2. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ Netlify (–æ–±—ã—á–Ω–æ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –≤ RSS)
3. –£–±–µ–¥–∏—Å—å, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç > 500 —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

1. –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã
2. –§—É–Ω–∫—Ü–∏—è `fetch_article_content()` —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
3. Web scraping –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ RSS
4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–±–æ–∏
5. –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è —É—Å–ø–µ—à–Ω–æ —Å –≤–Ω–µ—à–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
cd backend/news-admin
pip install -r requirements.txt
```

### –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–∏
```python
# –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
from backend.news_admin.index import fetch_article_content

url = "https://news.ycombinator.com/item?id=12345678"
content = fetch_article_content(url)
print(f"Length: {len(content)}")
print(f"Preview: {content[:200]}")
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î
```sql
SELECT 
    id,
    original_title,
    LENGTH(original_content) as orig_len,
    LENGTH(translated_content) as trans_len,
    source
FROM news 
WHERE source = 'Hacker News'
ORDER BY created_at DESC 
LIMIT 3;

-- –î–ª—è Hacker News –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è > 1000
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **Rate Limiting:** –ù–µ –¥–µ–ª–∞–π —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥
2. **User-Agent:** –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π User-Agent
3. **–¢–∞–π–º–∞—É—Ç—ã:** –í—Å–µ–≥–¥–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è requests
4. **–û—à–∏–±–∫–∏:** –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π –≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–π –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å
5. **Robots.txt:** –£–≤–∞–∂–∞–π –ø—Ä–∞–≤–∏–ª–∞ —Å–∞–π—Ç–æ–≤ (—Ö–æ—Ç—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π –æ–±—ã—á–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ)

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

**Hacker News (–±—ã–ª–æ):**
- `original_content`: "" (0 —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ RSS)

**Hacker News (—Å—Ç–∞–Ω–µ—Ç):**
- `original_content`: "Full article text from web..." (1000-5000 —Å–∏–º–≤–æ–ª–æ–≤)

**Netlify Blog (–±—ã–ª–æ –∏ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è):**
- `original_content`: "Full article from RSS..." (500-3000 —Å–∏–º–≤–æ–ª–æ–≤)

---

## ‚è≠Ô∏è –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø

–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–∞ 2 –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ **–≠—Ç–∞–ø—É 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞** –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ Ollama.

---

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –≠—Ç–æ—Ç —ç—Ç–∞–ø —Ä–µ—à–∞–µ—Ç –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 40% –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠—Ç–∞–ø 3 —É–ª—É—á—à–∏—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞.
